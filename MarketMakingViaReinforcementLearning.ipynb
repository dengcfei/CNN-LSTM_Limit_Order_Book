{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHEqJfGg3wJQ/uhmBDeDkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dengcfei/CNN-LSTM_Limit_Order_Book/blob/main/MarketMakingViaReinforcementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sab6l-iY0xAV",
        "outputId": "22683ac4-bb3a-464e-a4d0-a7332e41e624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0] 3.8.1 2.12.0\n"
          ]
        }
      ],
      "source": [
        "#check env\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "print(sys.version, nltk.__version__, tf.__version__, )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agent\n",
        "from tensorforce.agents import Agent\n",
        "\n",
        "def get_dueling_dqn_agent(\n",
        "                        network,\n",
        "                        environment=None,\n",
        "                        states=None,\n",
        "                        actions=None,\n",
        "                        max_episode_timesteps=None,\n",
        "                        batch_size=32,\n",
        "                        learning_rate=1e-4,\n",
        "                        horizon=1,\n",
        "                        discount=0.99,\n",
        "                        memory=200000,\n",
        "                        device='gpu'\n",
        "                        ):\n",
        "    if environment != None:\n",
        "        agent = Agent.create(\n",
        "        agent='dueling_dqn',\n",
        "        environment=environment,\n",
        "        max_episode_timesteps=max_episode_timesteps,\n",
        "        network=network,\n",
        "        config=dict(device=device),\n",
        "        memory=memory,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        horizon=horizon,\n",
        "        discount=discount,\n",
        "        parallel_interactions=10,\n",
        "    )\n",
        "    else:\n",
        "        agent = Agent.create(\n",
        "            agent='dueling_dqn',\n",
        "            states=states,\n",
        "            actions=actions,\n",
        "            max_episode_timesteps=max_episode_timesteps,\n",
        "            network=network,\n",
        "            config=dict(device=device),\n",
        "            memory=memory,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            horizon=horizon,\n",
        "            discount=discount,\n",
        "            parallel_interactions=10,\n",
        "        )\n",
        "    return agent\n",
        "\n",
        "def get_ppo_agent(\n",
        "                network,\n",
        "                environment=None,\n",
        "                states=None,\n",
        "                actions=None,\n",
        "                max_episode_timesteps=None,\n",
        "                batch_size=32,\n",
        "                learning_rate=1e-3,\n",
        "                horizon=None,\n",
        "                discount=0.99,\n",
        "                device='gpu'\n",
        "                ):\n",
        "    if environment != None:\n",
        "        agent = Agent.create(\n",
        "            agent='ppo',\n",
        "            environment=environment,\n",
        "            max_episode_timesteps=max_episode_timesteps,\n",
        "            network=network,\n",
        "            config=dict(device=device),\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            discount=discount,\n",
        "            parallel_interactions=10,\n",
        "        )\n",
        "    else:\n",
        "        agent = Agent.create(\n",
        "            agent='ppo',\n",
        "            environment=environment,\n",
        "            states=states,\n",
        "            actions=actions,\n",
        "            max_episode_timesteps=max_episode_timesteps,\n",
        "            network=network,\n",
        "            config=dict(device=device),\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            discount=discount,\n",
        "            parallel_interactions=10,\n",
        "        )\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "2Po5FSdW1uCb",
        "outputId": "8bda9382-f20b-49a6-a805-c44108278b27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eb1ef60e6fcd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m def get_dueling_dqn_agent(\n\u001b[1;32m      5\u001b[0m                         \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorforce'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorforce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3aW58cC2xa5",
        "outputId": "638d630b-d674-4c3c-8924-46398dd3843e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorforce\n",
            "  Using cached Tensorforce-0.6.5-py3-none-any.whl (309 kB)\n",
            "Requirement already satisfied: gym>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorforce) (0.25.2)\n",
            "Collecting h5py~=3.1.0 (from tensorforce)\n",
            "  Using cached h5py-3.1.0.tar.gz (371 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base_env\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from tensorforce import Environment\n",
        "\n",
        "from utils import day2date\n",
        "\n",
        "TRADE_UNIT = 100\n",
        "\n",
        "class BaseEnv():\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            initial_value=0,\n",
        "            max_episode_timesteps=1000,\n",
        "            data_dir='./data',\n",
        "            log=1,\n",
        "            experiment_name='',\n",
        "            **kwargs\n",
        "            ):\n",
        "        super().__init__()\n",
        "        self.name = ''\n",
        "        self.initial_value = initial_value\n",
        "        self.__max_episode_timesteps__=max_episode_timesteps\n",
        "        self.data_dir = data_dir\n",
        "        self.log = log\n",
        "        self.exp_name = experiment_name\n",
        "\n",
        "    '''\n",
        "        You need to overload these functions\n",
        "    '''\n",
        "\n",
        "    def states(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def actions(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def action2order(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_state_at_t(self, t):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_reward(self, trade_price, trade_volume):\n",
        "        # Define reward function here\n",
        "        reward = self.value - self.value_\n",
        "        self.value_ = self.value\n",
        "        return reward\n",
        "\n",
        "    '''\n",
        "        Load data\n",
        "    '''\n",
        "\n",
        "    def load_orderbook(self, code, day):\n",
        "        ask = pd.read_csv(self.data_dir + f'/{code}/{day}/ask.csv')\n",
        "        bid = pd.read_csv(self.data_dir + f'/{code}/{day}/bid.csv').drop(['timestamp'], axis = 1)\n",
        "\n",
        "        self.orderbook = pd.concat([ask, bid], axis=1)\n",
        "        self.orderbook.timestamp = pd.to_datetime(self.orderbook.timestamp)\n",
        "        self.orderbook = self.orderbook[(f'{self.day} 09:30:00'<self.orderbook.timestamp)&(self.orderbook.timestamp<f'{self.day} 14:57:00')]\n",
        "        self.orderbook = self.orderbook.set_index('timestamp')\n",
        "        self.orderbook_length = len(self.orderbook)\n",
        "        print('load lob done!', code, day)\n",
        "\n",
        "    def load_orderqueue(self, code, day):\n",
        "        pass\n",
        "\n",
        "    def load_price(self, code, day):\n",
        "        self.price = pd.read_csv(self.data_dir + f'/{code}/{day}/price.csv')\n",
        "        self.price.timestamp = pd.to_datetime(self.price.timestamp)\n",
        "        self.price = self.price.set_index('timestamp')\n",
        "        self.price = self.price.loc[self.orderbook.index]\n",
        "\n",
        "    def load_msg(self, code, day):\n",
        "        self.msg = pd.read_csv(self.data_dir + f'/{code}/{day}/msg.csv')\n",
        "        self.msg.timestamp = pd.to_datetime(self.msg.timestamp)\n",
        "        self.msg = self.msg.set_index('timestamp')\n",
        "        self.msg = self.msg.loc[self.orderbook.index]\n",
        "\n",
        "    def load_order(self, code, day):\n",
        "        order_columns = pd.read_csv('raw/GTA_SZL2_ORDER.csv')\n",
        "        self.order = pd.read_csv(f'raw/SZL2_ORDER_{code}_{day[:6]}.csv', names=list(order_columns), low_memory=False)\n",
        "        self.order.TradingTime = pd.to_datetime(self.order.TradingTime)\n",
        "        self.order = self.order[self.order.TradingDate==int(day)]\n",
        "        self.order = self.order[(f'{self.day} 09:30:00'<self.order.TradingTime)&(self.order.TradingTime<f'{self.day} 14:57:00')]\n",
        "\n",
        "    def load_trade(self, code, day):\n",
        "        trade_columns = pd.read_csv('raw/GTA_SZL2_TRADE.csv')\n",
        "        self.trade = pd.read_csv(f'raw/SZL2_TRADE_{code}_{day[:6]}.csv', names=list(trade_columns))\n",
        "        self.trade.TradingTime = pd.to_datetime(self.trade.TradingTime)\n",
        "        self.trade = self.trade[self.trade.TradingDate==int(day)]\n",
        "        self.trade = self.trade[self.trade.TradeType==\"F\"]\n",
        "        self.trade = self.trade[(f'{self.day} 09:30:00'<self.trade.TradingTime)&(self.trade.TradingTime<f'{self.day} 14:57:00')]\n",
        "\n",
        "        self.is_trade = pd.DataFrame(index=self.orderbook.index,columns=['is_trade'])\n",
        "        self.is_trade['is_trade'] = 0\n",
        "        self.is_trade.loc[set(self.trade.TradingTime)] = 1\n",
        "\n",
        "    '''\n",
        "        Common function\n",
        "    '''\n",
        "\n",
        "    def reset_seq(self, timesteps_per_episode=None, episode_idx=None):\n",
        "        self.episode_idx = episode_idx\n",
        "        if timesteps_per_episode == None:\n",
        "            self.episode_start = 0\n",
        "            self.episode_end = len(self.orderbook)\n",
        "            self.episode_state = self.orderbook\n",
        "        else:\n",
        "            self.episode_start = timesteps_per_episode * episode_idx\n",
        "            self.episode_end = min(self.episode_start + timesteps_per_episode, len(self.orderbook))\n",
        "            self.episode_state = self.orderbook.iloc[self.episode_start:self.episode_end]\n",
        "\n",
        "        self.episode_length = len(self.episode_state)\n",
        "\n",
        "        episode_is_trade = self.is_trade.iloc[self.episode_start:self.episode_end]\n",
        "        has_trade_index = np.where(episode_is_trade==1)[0]\n",
        "        has_trade_index = has_trade_index[has_trade_index>self.T]\n",
        "        self.index_iterator = iter(has_trade_index)\n",
        "\n",
        "        self.cash = self.value_ = self.value = self.initial_value\n",
        "        self.holding_pnl_total = self.trading_pnl_total = 0\n",
        "        self.inventory = 0\n",
        "        self.volume = 0\n",
        "        self.episode_reward = 0\n",
        "        self.mid_price_ = None\n",
        "        self.action_his = []\n",
        "        self.reward_dampened_pnl = 0\n",
        "        self.reward_trading_pnl = 0\n",
        "        self.reward_inventory_punishment = 0\n",
        "        self.reward_spread_punishment = 0\n",
        "\n",
        "        # log for trade\n",
        "        self.logger = self.price.iloc[self.episode_start:self.episode_end].copy()\n",
        "        columns=['ask_price', 'bid_price', 'trade_price', 'trade_volume', 'value', 'volume', 'cash', 'inventory']\n",
        "        for column in columns:\n",
        "            self.logger[column] = np.nan\n",
        "\n",
        "        self.i = next(self.index_iterator)\n",
        "        self.i_ = next(self.index_iterator)\n",
        "        state = self.get_state_at_t(self.i-self.latency)\n",
        "\n",
        "        if self.log >= 1:\n",
        "            print(f'Reset env {self.name} {self.code}, {self.day}, from {self.episode_state.index[0]} to {self.episode_state.index[-1]}')\n",
        "            self.pbar = tqdm(total=self.episode_length)\n",
        "            self.pbar.update(self.i)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def reset_random(self, timesteps_per_episode=2000):\n",
        "        self.episode_start = np.random.randint(0, len(self.orderbook) - timesteps_per_episode)\n",
        "        self.episode_end = min(self.episode_start + timesteps_per_episode, len(self.orderbook))\n",
        "        self.episode_state = self.orderbook.iloc[self.episode_start:self.episode_end]\n",
        "\n",
        "        self.episode_length = len(self.episode_state)\n",
        "\n",
        "        episode_is_trade = self.is_trade.iloc[self.episode_start:self.episode_end]\n",
        "        has_trade_index = np.where(episode_is_trade==1)[0]\n",
        "        has_trade_index = has_trade_index[has_trade_index>self.T]\n",
        "        self.index_iterator = iter(has_trade_index)\n",
        "\n",
        "        self.cash = self.value_ = self.value = self.initial_value\n",
        "        self.holding_pnl_total = self.trading_pnl_total = 0\n",
        "        self.inventory = 0\n",
        "        self.volume = 0\n",
        "        self.episode_reward = 0\n",
        "        self.mid_price_ = None\n",
        "        self.action_his = []\n",
        "        self.reward_dampened_pnl = 0\n",
        "        self.reward_trading_pnl = 0\n",
        "        self.reward_inventory_punishment = 0\n",
        "        self.reward_spread_punishment = 0\n",
        "\n",
        "        # log for trade\n",
        "        self.logger = self.price.iloc[self.episode_start:self.episode_end].copy()\n",
        "        columns=['ask_price', 'bid_price', 'trade_price', 'trade_volume', 'value', 'volume', 'cash', 'inventory']\n",
        "        for column in columns:\n",
        "            self.logger[column] = np.nan\n",
        "\n",
        "        self.i = next(self.index_iterator)\n",
        "        self.i_ = next(self.index_iterator)\n",
        "        state = self.get_state_at_t(self.i-self.latency)\n",
        "\n",
        "        if self.log:\n",
        "            print(f'Reset env {self.name} {self.code}, {self.day}, from {self.episode_state.index[0]} to {self.episode_state.index[-1]}')\n",
        "            self.pbar = tqdm(total=self.episode_length)\n",
        "            self.pbar.update(self.i)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def execute(self, actions):\n",
        "        self.action_his.append(actions)\n",
        "        # t\n",
        "        self.mid_price, self.ask1_price, self.bid1_price, self.lob_spread = self.get_price_info(self.i)\n",
        "        if self.mid_price_ == None:\n",
        "            self.mid_price_ = self.mid_price\n",
        "\n",
        "        orders = self.action2order(actions)\n",
        "        # inventory limit\n",
        "        if self.inventory < -10*TRADE_UNIT:\n",
        "            orders['ask_price']=0\n",
        "        elif self.inventory > 10*TRADE_UNIT:\n",
        "            orders['bid_price']=0\n",
        "\n",
        "        trade_price, trade_volume = self.match(orders)\n",
        "\n",
        "        self.update_agent(trade_price, trade_volume)\n",
        "\n",
        "        # log for trade result\n",
        "        self.logger.iloc[self.i, -8:] = [orders['ask_price'], orders['bid_price'], trade_price, trade_volume, self.value, self.volume, self.cash, self.inventory]\n",
        "\n",
        "        # if trade_volume:\n",
        "        #     print(self.i, 'ask1:', self.ask1_price, 'bid1:', self.bid1_price, 'buy' if trade_volume>0 else 'sell', 'at', trade_price)\n",
        "        if self.log >= 1:\n",
        "            self.pbar.update(self.i_ - self.i)\n",
        "\n",
        "        self.i = self.i_\n",
        "        # Termination conditions\n",
        "        terminal = False\n",
        "        try:\n",
        "            self.i_ = next(self.index_iterator)\n",
        "        except:\n",
        "            terminal = True\n",
        "\n",
        "        reward = self.get_reward(trade_price, trade_volume)\n",
        "        self.mid_price_ = self.mid_price\n",
        "\n",
        "        # close position\n",
        "        if terminal:\n",
        "            trade_price, trade_volume = self.close_position()\n",
        "            reward += self.get_reward(trade_price, trade_volume)\n",
        "\n",
        "        self.episode_reward += reward\n",
        "\n",
        "        # log for result\n",
        "        if terminal:\n",
        "            self.post_experiment()\n",
        "\n",
        "        state = self.get_state_at_t(self.i-self.latency)\n",
        "\n",
        "        return state, terminal, reward\n",
        "\n",
        "    def match(self, actions):\n",
        "        trade_volume = 0\n",
        "        trade_price = 0\n",
        "        ask_price, ask_volume, bid_price, bid_volume = actions.values()\n",
        "\n",
        "        # trade\n",
        "        now_t = self.trade[self.trade.TradingTime==self.episode_state.index[self.i]]\n",
        "        now_trading_price_max = now_t.TradePrice.max()\n",
        "        now_trading_price_max_v = now_t[now_t.TradePrice==now_trading_price_max].TradeVolume.sum()\n",
        "        now_trading_price_min = now_t.TradePrice.min()\n",
        "        now_trading_price_min_v = now_t[now_t.TradePrice==now_trading_price_min].TradeVolume.sum()\n",
        "\n",
        "        # t - 1\n",
        "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-1)\n",
        "\n",
        "        # sell order\n",
        "        if ask_price and ask_volume:\n",
        "            if ask_price <= t_1_b1_price:\n",
        "                # market order\n",
        "                trade_price, trade_volume = t_1_b1_price, ask_volume\n",
        "                # print(\"market order sell at\", trade_price)\n",
        "            else:\n",
        "                # limit order\n",
        "                if now_trading_price_max > ask_price:\n",
        "                    # all deal\n",
        "                    trade_price, trade_volume = ask_price, ask_volume\n",
        "                    # print(\"limit order sell at\", trade_price)\n",
        "\n",
        "                # we assume that our quotes rest at the back of the queue\n",
        "                elif now_trading_price_max == ask_price:\n",
        "                    # deal probability: traded volume/all volume in this level\n",
        "                    lob_depth = self.episode_state.iloc[self.i].ask1_volume\n",
        "                    transac_prob = now_trading_price_max_v/(now_trading_price_max_v+lob_depth)\n",
        "                    is_transac = np.random.choice([1, 0], p=[transac_prob, 1-transac_prob])\n",
        "                    if is_transac:\n",
        "                        trade_price, trade_volume = ask_price, ask_volume\n",
        "\n",
        "        # buy order\n",
        "        if bid_price and bid_volume:\n",
        "            if bid_price >= t_1_a1_price:\n",
        "                # market order\n",
        "                trade_price, trade_volume = t_1_a1_price, bid_volume\n",
        "                # print(\"market order buy at\", trade_price)\n",
        "            else:\n",
        "                if now_trading_price_min < bid_price:\n",
        "                    trade_price, trade_volume = bid_price, bid_volume\n",
        "                    # print(\"limit order buy at\", trade_price)\n",
        "\n",
        "                # we assume that our quotes rest at the back of the queue\n",
        "                elif now_trading_price_min == bid_price:\n",
        "                    lob_depth = self.episode_state.iloc[self.i].bid1_volume\n",
        "                    transac_prob = now_trading_price_min_v/(now_trading_price_min_v+lob_depth)\n",
        "                    is_transac = np.random.choice([1, 0], p=[transac_prob, 1-transac_prob])\n",
        "                    if is_transac:\n",
        "                        trade_price, trade_volume = bid_price, bid_volume\n",
        "\n",
        "        return trade_price, trade_volume\n",
        "\n",
        "    def close_position(self):\n",
        "        # t - 1\n",
        "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-1)\n",
        "\n",
        "        # Market order\n",
        "        if self.inventory < 0:\n",
        "            # Buy\n",
        "            trade_price, trade_volume = t_1_a1_price, -self.inventory\n",
        "            self.volume += trade_volume\n",
        "        elif self.inventory > 0:\n",
        "            # Sell\n",
        "            trade_price, trade_volume = t_1_b1_price, -self.inventory\n",
        "        else:\n",
        "            trade_price, trade_volume = 0, 0\n",
        "\n",
        "        self.update_agent(trade_price, trade_volume)\n",
        "\n",
        "        # log for trade result\n",
        "        self.logger.iloc[self.i, -6:] = [trade_price, trade_volume, self.value, self.volume, self.cash, self.inventory]\n",
        "\n",
        "        return trade_price, trade_volume\n",
        "\n",
        "    def update_agent(self, trade_price, trade_volume):\n",
        "        self.inventory_ = self.inventory\n",
        "        self.inventory += trade_volume\n",
        "        self.cash -= trade_volume*trade_price\n",
        "        self.value = self.get_value(self.mid_price)\n",
        "\n",
        "        volume = max(0, trade_volume*trade_price) # only count for buy\n",
        "        self.volume += volume\n",
        "\n",
        "    def get_price_info(self, i):\n",
        "        price = self.price[self.price.index==self.episode_state.index[i]]\n",
        "\n",
        "        bid1_price = price.bid1_price.item()\n",
        "        ask1_price = price.ask1_price.item()\n",
        "        bid1_price, ask1_price = round(bid1_price,2), round(ask1_price,2)\n",
        "        mid_price = (bid1_price+ask1_price)/2\n",
        "        spread = ask1_price - bid1_price\n",
        "\n",
        "        return mid_price, ask1_price, bid1_price, spread\n",
        "\n",
        "    def get_value(self, price):\n",
        "        return self.cash + self.inventory*price\n",
        "\n",
        "    '''\n",
        "        For evaluation and save trading log\n",
        "    '''\n",
        "\n",
        "    def post_experiment(self, save=False):\n",
        "        logger_wo_exit_market = self.logger[(self.logger.ask_price != 0) & (self.logger.bid_price != 0)]\n",
        "        self.episode_avg_spread = (logger_wo_exit_market.ask_price - logger_wo_exit_market.bid_price).mean()\n",
        "        self.episode_avg_position = self.logger.inventory.mean()\n",
        "        self.episode_avg_abs_position = self.logger.inventory.abs().mean()\n",
        "        self.episode_profit_ratio = self.value/(self.volume+1e-7)\n",
        "        self.pnl = self.value - self.initial_value\n",
        "        self.nd_pnl = self.pnl/self.episode_avg_spread\n",
        "        self.pnl_map = self.pnl/(self.episode_avg_abs_position+1e-7)\n",
        "\n",
        "        if self.log >= 1:\n",
        "            print(\n",
        "                \"PnL:\", self.pnl,\n",
        "                \"Holding PnL\", self.holding_pnl_total,\n",
        "                \"Trading PnL\", self.trading_pnl_total,\n",
        "                \"ND-PnL:\", self.nd_pnl,\n",
        "                \"PnL-MAP:\", self.pnl_map,\n",
        "                \"Trading volume:\", self.volume,\n",
        "                \"Profit ratio:\", self.episode_profit_ratio,\n",
        "                \"Averaged position:\",self.episode_avg_position,\n",
        "                \"Averaged Abs position:\",self.episode_avg_abs_position,\n",
        "                \"Averaged spread:\", self.episode_avg_spread,\n",
        "                \"Episodic reward:\", self.episode_reward\n",
        "                )\n",
        "            self.pbar.close()\n",
        "\n",
        "        if self.log >= 2:\n",
        "            trade_log = self.logger[(self.logger.trade_volume > 0)|(self.logger.trade_volume < 0)]\n",
        "            for i in range(len(trade_log)):\n",
        "                item = trade_log.iloc[i]\n",
        "                if item.trade_volume > 0:\n",
        "                    print(item.name, 'BUY at', item.trade_price, 'inventory', item.inventory, 'value', item.value)\n",
        "                elif item.trade_volume < 0:\n",
        "                    print(item.name, 'SELL at', item.trade_price, 'inventory', item.inventory, 'value', item.value)\n",
        "\n",
        "        if save:\n",
        "            now_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime())\n",
        "            log_file = f\"./log/{self.exp_name}_{self.code}_{self.day}_{now_time}.csv\"\n",
        "            self.logger.to_csv(log_file)\n",
        "            print(\"Trading log saved to\", log_file)\n",
        "\n",
        "    def get_final_result(self):\n",
        "        return dict(\n",
        "            pnl=self.pnl,\n",
        "            nd_pnl=self.nd_pnl,\n",
        "            pnl_map=self.pnl_map,\n",
        "            profit_ratio=self.episode_profit_ratio,\n",
        "            avg_position=self.episode_avg_position,\n",
        "            avg_abs_position=self.episode_avg_abs_position,\n",
        "            avg_spread=self.episode_avg_spread,\n",
        "            volume=self.volume,\n",
        "            episode_reward=self.episode_reward\n",
        "        )"
      ],
      "metadata": {
        "id": "-5ac14X92z82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "90Kcyv494AY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env_continuous\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "from .env_feature import EnvFeature\n",
        "from .base_env import TRADE_UNIT\n",
        "\n",
        "from utils import day2date, lob_norm, price_legal_check\n",
        "\n",
        "class EnvContinuous(EnvFeature):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            code='000001',\n",
        "            day='20191101',\n",
        "            latency=1,\n",
        "            T=50,\n",
        "            # ablation states\n",
        "            wo_lob_state=False,\n",
        "            wo_market_state=False,\n",
        "            wo_agent_state=False,\n",
        "            # ablation rewards\n",
        "            wo_dampened_pnl=False,\n",
        "            wo_matched_pnl=False,\n",
        "            wo_inv_punish=False,\n",
        "            **kwargs\n",
        "        ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.name = \"Continuous\"\n",
        "        print(\"Environment:\", self.name)\n",
        "        self.code = code\n",
        "        self.day = day2date(day)\n",
        "\n",
        "        self.latency = latency\n",
        "        self.T = T\n",
        "\n",
        "        # ablation\n",
        "        self.wo_lob_state = wo_lob_state\n",
        "        self.wo_market_state = wo_market_state\n",
        "        self.wo_agent_state = wo_agent_state\n",
        "        self.r_da = 0 if wo_dampened_pnl else 1\n",
        "        self.r_ma = 0 if wo_matched_pnl else 1\n",
        "        self.r_ip = 0 if wo_inv_punish else 1\n",
        "\n",
        "        # Inventory punishment factor\n",
        "        self.theta = 0.01\n",
        "        self.eta = 0.9\n",
        "\n",
        "        self.init_states()\n",
        "\n",
        "        self.load_orderbook(code=code, day=day)\n",
        "        self.load_price(code=code, day=day)\n",
        "        self.load_trade(code=code, day=day)\n",
        "        self.load_msg(code=code, day=day)\n",
        "\n",
        "    def init_states(self):\n",
        "        self.__states_space__ = dict()\n",
        "        if not self.wo_lob_state:\n",
        "            self.__states_space__['lob_state'] = dict(\n",
        "                type='float',\n",
        "                shape=(self.T,40,1)\n",
        "                )\n",
        "        if not self.wo_market_state:\n",
        "            self.__states_space__['market_state'] = dict(\n",
        "                type='float',\n",
        "                shape=(24,)\n",
        "                )\n",
        "        if not self.wo_agent_state:\n",
        "            self.__states_space__['agent_state'] = dict(\n",
        "                type='float',\n",
        "                shape=(24,)\n",
        "                )\n",
        "\n",
        "    def states(self):\n",
        "        return self.__states_space__\n",
        "\n",
        "    def actions(self):\n",
        "        return dict(\n",
        "                    type='float',\n",
        "                    shape=(2,),\n",
        "                    min_value=-1,\n",
        "                    max_value=1\n",
        "                )\n",
        "\n",
        "    def max_episode_timesteps(self):\n",
        "        return self.__max_episode_timesteps__\n",
        "\n",
        "    def action2order(self, actions):\n",
        "        # t-latency\n",
        "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-self.latency)\n",
        "\n",
        "        # action 1\n",
        "        # actions in [0, 1]\n",
        "        delta_price = actions[0]*0.05\n",
        "        spread = actions[1]*0.1\n",
        "        if self.inventory > 0:\n",
        "            reservation = t_1_mid_price - delta_price\n",
        "        elif self.inventory < 0:\n",
        "            reservation = t_1_mid_price + delta_price\n",
        "        else:\n",
        "            reservation = t_1_mid_price\n",
        "        ask_price = reservation + spread/2\n",
        "        bid_price = reservation - spread/2\n",
        "\n",
        "        # action 2\n",
        "        # actions in [-1, 1]\n",
        "        # delta_price = actions[0]*0.05\n",
        "        # spread = abs(actions[1])*0.1\n",
        "        # reservation = t_1_mid_price - delta_price\n",
        "        # ask_price = reservation + spread/2\n",
        "        # bid_price = reservation - spread/2\n",
        "\n",
        "        # action 3\n",
        "        # actions in [0, 1]\n",
        "        # ask_price = t_1_a1_price + actions[0]*0.1\n",
        "        # bid_price = t_1_b1_price - actions[1]*0.1\n",
        "        # reservation = (ask_price + bid_price)/2\n",
        "        # spread = ask_price - bid_price\n",
        "\n",
        "        ask_price, bid_price = price_legal_check(ask_price, bid_price)\n",
        "\n",
        "        # save for log\n",
        "        self.reservation = reservation\n",
        "        self.spread = spread\n",
        "\n",
        "        orders = {\n",
        "            'ask_price': ask_price,\n",
        "            'ask_vol': -TRADE_UNIT,\n",
        "            'bid_price': bid_price,\n",
        "            'bid_vol': TRADE_UNIT\n",
        "        }\n",
        "        return orders\n",
        "\n",
        "    def get_reward(self, trade_price, trade_volume):\n",
        "        pnl = self.value - self.value_\n",
        "\n",
        "        # Asymmetrically dampened PnL\n",
        "        asymmetric_dampen = max(0, self.eta * pnl)\n",
        "        dampened_pnl = pnl - asymmetric_dampen\n",
        "\n",
        "        matched_pnl = (self.mid_price - trade_price) * trade_volume\n",
        "\n",
        "        # delta_inventory = abs(self.inventory) - abs(self.inventory_)\n",
        "        # delta_inventory = max(0, delta_inventory)\n",
        "        # inventory_punishment = self.theta * (delta_inventory/TRADE_UNIT)\n",
        "\n",
        "        inventory_punishment = self.theta * (self.inventory/TRADE_UNIT)**2\n",
        "\n",
        "        # spread punishment\n",
        "        if self.inventory:\n",
        "            spread_punishment = 0\n",
        "        else:\n",
        "            spread_punishment = 100*self.spread if self.spread > 0.02 else 0\n",
        "\n",
        "        reward = pnl - spread_punishment#self.r_ma * matched_pnl + self.r_da * dampened_pnl - self.r_ip * inventory_punishment - spread_punishment\n",
        "\n",
        "        self.value_ = self.value\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_state_at_t(self, t):\n",
        "        self.__state__ = dict()\n",
        "\n",
        "        if not self.wo_lob_state:\n",
        "            lob = self.episode_state.iloc[t-self.T:t]\n",
        "            mid_price = (lob.ask1_price + lob.bid1_price)/2\n",
        "            lob_normed = lob_norm(lob, mid_price)\n",
        "            self.__state__['lob_state'] = np.expand_dims(np.array(lob_normed), -1)\n",
        "\n",
        "        if not self.wo_market_state:\n",
        "            self.__state__['market_state'] = self._get_market_state(t) + self._get_order_strength_index(t)\n",
        "\n",
        "        if not self.wo_agent_state:\n",
        "            self.__state__['agent_state'] = [self.inventory/(10*TRADE_UNIT)]*12 + [t / self.episode_length]*12\n",
        "\n",
        "        return self.__state__"
      ],
      "metadata": {
        "id": "dmblnyZf3S_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#env_discret\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "from .env_feature import EnvFeature\n",
        "from .base_env import TRADE_UNIT\n",
        "\n",
        "from utils import day2date, lob_norm\n",
        "\n",
        "class EnvDiscrete(EnvFeature):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            code='000001',\n",
        "            day='20191101',\n",
        "            data_norm=True,\n",
        "            latency=1,\n",
        "            T=50,\n",
        "            # ablation states\n",
        "            wo_lob_state=False,\n",
        "            wo_market_state=False,\n",
        "            wo_agent_state=False,\n",
        "            # ablation rewards\n",
        "            wo_dampened_pnl=False,\n",
        "            wo_matched_pnl=False,\n",
        "            wo_inv_punish=False,\n",
        "            **kwargs\n",
        "        ):\n",
        "        super().__init__(**kwargs)\n",
        "        print(\"Environment: EnvDiscrete\")\n",
        "        self.code = code\n",
        "        self.day = day2date(day)\n",
        "\n",
        "        self.latency = latency\n",
        "        self.T = T\n",
        "\n",
        "        # ablation\n",
        "        self.wo_lob_state = wo_lob_state\n",
        "        self.wo_market_state = wo_market_state\n",
        "        self.wo_agent_state = wo_agent_state\n",
        "        self.r_da = 0 if wo_dampened_pnl else 1\n",
        "        self.r_ma = 0 if wo_matched_pnl else 1\n",
        "        self.r_ip = 0 if wo_inv_punish else 1\n",
        "\n",
        "        # Inventory punishment factor\n",
        "        self.theta = 0.01\n",
        "        self.eta = 0.5\n",
        "\n",
        "        self.init_states()\n",
        "\n",
        "        self.load_orderbook(code=code, day=day)\n",
        "        self.load_price(code=code, day=day)\n",
        "        self.load_trade(code=code, day=day)\n",
        "        self.load_msg(code=code, day=day)\n",
        "\n",
        "    def init_states(self):\n",
        "        self.__states_space__ = dict()\n",
        "        if not self.wo_lob_state:\n",
        "            self.__states_space__['lob_state'] = dict(\n",
        "                type='float',\n",
        "                shape=(self.T,40,1)\n",
        "                )\n",
        "        if not self.wo_market_state:\n",
        "            self.__states_space__['market_state'] = dict(\n",
        "                type='float',\n",
        "                shape=(24,)\n",
        "                )\n",
        "        if not self.wo_agent_state:\n",
        "            self.__states_space__['agent_state'] = dict(\n",
        "                type='float',\n",
        "                shape=(24,)\n",
        "                )\n",
        "\n",
        "    def states(self):\n",
        "        return self.__states_space__\n",
        "\n",
        "    def actions(self):\n",
        "        return dict(\n",
        "                    type='int',\n",
        "                    num_values=5\n",
        "                )\n",
        "\n",
        "    def max_episode_timesteps(self):\n",
        "        return self.__max_episode_timesteps__\n",
        "\n",
        "    def action2order(self, actions):\n",
        "        # t-latency\n",
        "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-self.latency)\n",
        "\n",
        "        ask_price, bid_price = 0, 0\n",
        "        ask_volume, bid_volume = -TRADE_UNIT,TRADE_UNIT\n",
        "\n",
        "        if actions in range(7):\n",
        "            # limit order\n",
        "            if actions == 0:\n",
        "                ask_price = t_1_a1_price\n",
        "                bid_price = t_1_b1_price\n",
        "            elif actions == 1:\n",
        "                ask_price = t_1_a1_price\n",
        "                bid_price = t_1_b1_price-0.01\n",
        "            elif actions == 2:\n",
        "                ask_price = t_1_a1_price+0.01\n",
        "                bid_price = t_1_b1_price\n",
        "            elif actions == 3:\n",
        "                ask_price = t_1_a1_price+0.01\n",
        "                bid_price = t_1_b1_price-0.01\n",
        "            elif actions == 4:\n",
        "                ask_price = t_1_a1_price\n",
        "                bid_price = t_1_b1_price-0.02\n",
        "            elif actions == 5:\n",
        "                ask_price = t_1_a1_price+0.02\n",
        "                bid_price = t_1_b1_price\n",
        "            elif actions == 6:\n",
        "                ask_price = t_1_a1_price+0.02\n",
        "                bid_price = t_1_b1_price-0.02\n",
        "\n",
        "        elif actions==7:\n",
        "            # market order to clode position\n",
        "            if self.inventory < 0:\n",
        "                bid_price, bid_volume = np.inf, -self.inventory\n",
        "            elif self.inventory > 0:\n",
        "                ask_price, ask_volume = 0.01, -self.inventory\n",
        "            else:\n",
        "                trade_price, trade_volume = 0, 0\n",
        "\n",
        "        # inventory limit\n",
        "        if self.inventory < -10*TRADE_UNIT:\n",
        "            ask_price=0\n",
        "            ask_volume=0\n",
        "        elif self.inventory > 10*TRADE_UNIT:\n",
        "            bid_price=0\n",
        "            bid_volume=0\n",
        "\n",
        "        orders = {\n",
        "            'ask_price': ask_price,\n",
        "            'ask_vol': ask_volume,\n",
        "            'bid_price': bid_price,\n",
        "            'bid_vol': bid_volume\n",
        "        }\n",
        "\n",
        "        return orders\n",
        "\n",
        "    def get_reward(self, trade_price, trade_volume):\n",
        "        pnl = self.value - self.value_\n",
        "\n",
        "        # Asymmetrically dampened PnL\n",
        "        asymmetric_dampen = max(0, self.eta * pnl)\n",
        "        dampened_pnl = pnl - asymmetric_dampen\n",
        "\n",
        "        matched_pnl = (self.mid_price - trade_price) * trade_volume\n",
        "\n",
        "        delta_inventory = abs(self.inventory) - abs(self.inventory_)\n",
        "        # delta_inventory = max(0, delta_inventory)\n",
        "\n",
        "        inventory_punishment = self.theta * (delta_inventory/TRADE_UNIT)\n",
        "        # inventory_punishment = self.theta * (self.inventory/TRADE_UNIT)**2\n",
        "        reward = pnl\n",
        "        # reward = self.r_ma * matched_pnl + self.r_da * dampened_pnl - self.r_ip * inventory_punishment\n",
        "        self.value_ = self.value\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_state_at_t(self, t):\n",
        "        self.__state__ = dict()\n",
        "\n",
        "        if not self.wo_lob_state:\n",
        "            lob = self.episode_state.iloc[t-self.T:t]\n",
        "            mid_price = (lob.ask1_price + lob.bid1_price)/2\n",
        "            lob_normed = lob_norm(lob, mid_price)\n",
        "            self.__state__['lob_state'] = np.expand_dims(np.array(lob_normed), -1)\n",
        "\n",
        "        if not self.wo_market_state:\n",
        "            self.__state__['market_state'] = self._get_market_state(t) + self._get_order_strength_index(t)\n",
        "\n",
        "        if not self.wo_agent_state:\n",
        "            self.__state__['agent_state'] = [self.inventory/(10*TRADE_UNIT)]*12 + [t / self.episode_length]*12\n",
        "\n",
        "        return self.__state__"
      ],
      "metadata": {
        "id": "ZTBvC4jj3Wrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#env_feature\n",
        "from datetime import timedelta\n",
        "\n",
        "from .base_env import BaseEnv\n",
        "\n",
        "from utils import getRealizedVolatility, getRelativeStrengthIndex, getOrderStrengthIndex\n",
        "\n",
        "class EnvFeature(BaseEnv):\n",
        "    \"\"\"\n",
        "        Use this class to calculate your factor\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            **kwargs\n",
        "        ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def _get_market_state(self,t):\n",
        "        data_300s = self.price[(self.price.index<=self.episode_state.index[t])&(self.price.index>=self.episode_state.index[t]-timedelta(seconds=300))].midprice\n",
        "        data_600s = self.price[(self.price.index<=self.episode_state.index[t])&(self.price.index>=self.episode_state.index[t]-timedelta(seconds=600))].midprice\n",
        "        data_1800s = self.price[(self.price.index<=self.episode_state.index[t])&(self.price.index>=self.episode_state.index[t]-timedelta(seconds=1800))].midprice\n",
        "        rv_300s = getRealizedVolatility(data_300s,resample='s')*1e4\n",
        "        rv_600s = getRealizedVolatility(data_600s,resample='s')*1e4\n",
        "        rv_1800s = getRealizedVolatility(data_1800s,resample='s')*1e4\n",
        "        rsi_300s = getRelativeStrengthIndex(data_300s)\n",
        "        rsi_600s = getRelativeStrengthIndex(data_600s)\n",
        "        rsi_1800s = getRelativeStrengthIndex(data_1800s)\n",
        "        return [rv_300s, rv_600s, rv_1800s, rsi_300s, rsi_600s, rsi_1800s]\n",
        "\n",
        "    def _get_order_strength_index(self,t):\n",
        "        data_10s = self.msg[(self.msg.index<=self.episode_state.index[t])&(self.msg.index>=self.episode_state.index[t]-timedelta(seconds=10))]\n",
        "        data_60s = self.msg[(self.msg.index<=self.episode_state.index[t])&(self.msg.index>=self.episode_state.index[t]-timedelta(seconds=60))]\n",
        "        data_300s = self.msg[(self.msg.index<=self.episode_state.index[t])&(self.msg.index>=self.episode_state.index[t]-timedelta(seconds=300))]\n",
        "\n",
        "        svi_10s, sni_10s, lvi_10s, lni_10s, wvi_10s, wni_10s = getOrderStrengthIndex(data_10s)\n",
        "        svi_60s, sni_60s, lvi_60s, lni_60s, wvi_60s, wni_60s = getOrderStrengthIndex(data_60s)\n",
        "        svi_300s, sni_300s, lvi_300s, lni_300s, wvi_300s, wni_300s = getOrderStrengthIndex(data_300s)\n",
        "\n",
        "        return [\n",
        "            svi_10s, sni_10s, lvi_10s, lni_10s, wvi_10s, wni_10s,\n",
        "            svi_60s, sni_60s, lvi_60s, lni_60s, wvi_60s, wni_60s,\n",
        "            svi_300s, sni_300s, lvi_300s, lni_300s, wvi_300s, wni_300s\n",
        "        ]"
      ],
      "metadata": {
        "id": "ujGVWrkW3b11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#network\n",
        "# import os\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True                                #按需分配显存\n",
        "K.set_session(tf.compat.v1.Session(config=config))\n",
        "\n",
        "def get_lob_model(latent_dim, T):\n",
        "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
        "\n",
        "    conv_first1 = keras.layers.Conv2D(32, (1, 2), strides=(1, 2))(lob_state)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    conv_first1 = keras.layers.Conv2D(32, (1, 5), strides=(1, 5))(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    conv_first1 = keras.layers.Conv2D(32, (1, 4))(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    # build the inception module\n",
        "    convsecond_1 = keras.layers.Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
        "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
        "    convsecond_1 = keras.layers.Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
        "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
        "\n",
        "    convsecond_2 = keras.layers.Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
        "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
        "    convsecond_2 = keras.layers.Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
        "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
        "\n",
        "    convsecond_3 = keras.layers.MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
        "    convsecond_3 = keras.layers.Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
        "    convsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
        "\n",
        "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
        "    conv_reshape = keras.layers.Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
        "\n",
        "    attn_input = conv_reshape\n",
        "    attn_input_last = attn_input[:,-1:,:]\n",
        "\n",
        "    multi_head_attn_layer_1 = keras.layers.MultiHeadAttention(num_heads=10, key_dim=16, output_shape=64)\n",
        "\n",
        "    attn_output, weight = multi_head_attn_layer_1(attn_input_last, attn_input, return_attention_scores=True)\n",
        "\n",
        "    attn_output = keras.layers.Flatten()(attn_output)\n",
        "\n",
        "    # add Batch Normalization\n",
        "    # attn_output = keras.layers.BatchNormalization()(attn_output)\n",
        "\n",
        "    # add Layer Normalization\n",
        "    # attn_output = keras.layers.LayerNormalization()(attn_output)\n",
        "\n",
        "    return keras.models.Model(lob_state, attn_output)\n",
        "\n",
        "\n",
        "def get_fclob_model(latent_dim,T):\n",
        "    print(\"This is the FC-LOB model\")\n",
        "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
        "\n",
        "    dense_input = keras.layers.Flatten()(lob_state)\n",
        "\n",
        "    dense_output = keras.layers.Dense(1024, activation='leaky_relu')(dense_input)\n",
        "    dense_output = keras.layers.Dense(256, activation='leaky_relu')(dense_input)\n",
        "    dense_output = keras.layers.Dense(latent_dim, activation='leaky_relu')(dense_input)\n",
        "\n",
        "    return keras.models.Model(lob_state, dense_output)\n",
        "\n",
        "def compute_output_shape(input_shape):\n",
        "    return (input_shape[0], 64)\n",
        "\n",
        "def get_pretrain_model(model, T):\n",
        "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
        "    embedding = model(lob_state)\n",
        "    output = keras.layers.Dense(3, activation='softmax')(embedding)\n",
        "\n",
        "    return keras.models.Model(lob_state, output)\n",
        "\n",
        "def get_model(lob_model, T, with_lob_state=True, with_market_state=True, with_agent_state=True):\n",
        "    input_ls = list()\n",
        "    dense_input = list()\n",
        "    if with_lob_state:\n",
        "        lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
        "        encoder_outputs = lob_model(lob_state)\n",
        "        input_ls.append(lob_state)\n",
        "        dense_input.append(encoder_outputs)\n",
        "    else:\n",
        "        print('w/o lob state!')\n",
        "\n",
        "    if with_agent_state:\n",
        "        agent_state = keras.layers.Input(shape=(24,))\n",
        "        input_ls.append(agent_state)\n",
        "        dense_input.append(agent_state)\n",
        "    else:\n",
        "         print('w/o agent state!')\n",
        "\n",
        "    if with_market_state:\n",
        "        market_state = keras.layers.Input(shape=(24,))\n",
        "        input_ls.append(market_state)\n",
        "        dense_input.append(agent_state)\n",
        "    else:\n",
        "        print('w/o market state!')\n",
        "\n",
        "    dense_input = keras.layers.concatenate(dense_input, axis=1)\n",
        "\n",
        "    dense_output = keras.layers.Dense(64, activation='leaky_relu')(dense_input)\n",
        "\n",
        "    return keras.models.Model(input_ls, dense_output)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    get_lob_model(64,50).summary()"
      ],
      "metadata": {
        "id": "esR6R0cR3kXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utility\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def min_max_norm(data):\n",
        "    minVals = data.min()\n",
        "    maxVals = data.max()\n",
        "    ranges = maxVals - minVals\n",
        "    if ranges==0:\n",
        "        return data*0\n",
        "    normData = data - minVals\n",
        "    normData = normData/ranges\n",
        "    return normData\n",
        "\n",
        "def z_norm(data):\n",
        "    return (data-data.mean())/(data.std()+1e-7)\n",
        "\n",
        "def lob_norm(data_, midprice):\n",
        "    data = data_.copy()\n",
        "    for i in range(10):\n",
        "        data[f'ask{i+1}_price'] = data[f'ask{i+1}_price']/(midprice+1e-7) - 1\n",
        "        data[f'bid{i+1}_price'] = data[f'bid{i+1}_price']/(midprice+1e-7) - 1\n",
        "        # data[f'ask{i+1}_price'] = z_norm(data[f'ask{i+1}_price'])\n",
        "        # data[f'bid{i+1}_price'] = z_norm(data[f'bid{i+1}_price'])\n",
        "        data[f'ask{i+1}_volume'] = data[f'ask{i+1}_volume']/data[f'ask{i+1}_volume'].max()\n",
        "        data[f'bid{i+1}_volume'] = data[f'bid{i+1}_volume']/data[f'bid{i+1}_volume'].max()\n",
        "\n",
        "    return data\n",
        "\n",
        "def onehot_label(targets):\n",
        "    from tensorflow import keras\n",
        "    # targets: pd.DataFrame len(data)*n_horizons\n",
        "    all_label = []\n",
        "    for i in range(targets.shape[1]):\n",
        "        label = targets.iloc[:,i] - 1\n",
        "        label = keras.utils.to_categorical(label, 3)\n",
        "        # label = label.reshape(len(label), 1, 3)\n",
        "        all_label.append(label)\n",
        "    return np.hstack(all_label)\n",
        "\n",
        "def day2date(day):\n",
        "    day = list(day)\n",
        "    day.insert(4,'-')\n",
        "    day.insert(7,'-')\n",
        "    date = ''.join(day)\n",
        "    return date\n",
        "\n",
        "def pd_is_equal(state_1, state_2):\n",
        "    tmp_1 = state_1.iloc[:,1:]\n",
        "    tmp_2 = state_2.iloc[:,1:]\n",
        "    return tmp_1.equals(tmp_2)\n",
        "\n",
        "def load_data(code, datelist, horizon=10):\n",
        "    if type(datelist) is str:\n",
        "        datelist = [datelist]\n",
        "    data_list = []\n",
        "    for day in datelist:\n",
        "        ask = pd.read_csv(f\"data/{code}/{day}/ask.csv\")\n",
        "        bid = pd.read_csv(f\"data/{code}/{day}/bid.csv\").drop(['timestamp'], axis = 1)\n",
        "        price = pd.read_csv(f\"data/{code}/{day}/price.csv\").drop(['timestamp', 'ask1_price', 'bid1_price'], axis = 1)\n",
        "        data = pd.concat([ask, bid, price], axis=1)\n",
        "        data['date'] = data['timestamp'].str.split(expand=True)[0]\n",
        "        data['time'] = data['timestamp'].str.split(expand=True)[1]\n",
        "        data.drop('timestamp', axis=1, inplace=True)\n",
        "\n",
        "        data['y']=getLabel(data.midprice, horizon)\n",
        "\n",
        "        data_list.append(data)\n",
        "    return pd.concat(data_list)\n",
        "\n",
        "def getLabel(mid_price, horizon, threshold=1e-5):\n",
        "    price_past = mid_price.rolling(window=horizon).mean()\n",
        "\n",
        "    price_future = mid_price.copy()\n",
        "    price_future[:-horizon] = price_past[horizon:]\n",
        "    price_future[-horizon:] = np.nan\n",
        "\n",
        "    pct_change = (price_future - price_past)/price_past\n",
        "    pct_change[pct_change>=threshold] = 1\n",
        "    pct_change[(pct_change<threshold) & (-threshold<pct_change)] = 2\n",
        "    pct_change[pct_change<=-threshold] = 3\n",
        "    return pct_change\n",
        "\n",
        "def process_data(data):\n",
        "    data = data[(data.time > '10:00:00')&(data.time < '14:30:00')]\n",
        "    data = data.dropna()\n",
        "    data.y = data.y.astype(int)\n",
        "\n",
        "    for i in range(10):\n",
        "        data[f'ask{i+1}_price'] = data[f'ask{i+1}_price']/data['midprice'] - 1\n",
        "        data[f'bid{i+1}_price'] = data[f'bid{i+1}_price']/data['midprice'] - 1\n",
        "        # data[f'ask{i+1}_price'] = z_norm(data[f'ask{i+1}_price'])\n",
        "        # data[f'bid{i+1}_price'] = z_norm(data[f'bid{i+1}_price'])\n",
        "        data[f'ask{i+1}_volume'] = data[f'ask{i+1}_volume']/data[f'ask{i+1}_volume'].max()\n",
        "        data[f'bid{i+1}_volume'] = data[f'bid{i+1}_volume']/data[f'bid{i+1}_volume'].max()\n",
        "\n",
        "    return data.set_index(['date', 'time'])\n",
        "\n",
        "def reorder(data):\n",
        "    '''\n",
        "    reorder the data to this order:\n",
        "    ask1_v, ask1_p, bid1_v, bid1_p ... ask10_v, ask10_p, bid10_v, bid10_p\n",
        "    '''\n",
        "    data=np.array(data)\n",
        "    data=data.reshape(data.shape[0], 4, 10)\n",
        "    data= np.transpose(data, (0,2,1))\n",
        "    data = data.reshape(data.shape[0], -1)\n",
        "    return data\n",
        "\n",
        "def data_classification(X, Y, T):\n",
        "    [N, D] = X.shape\n",
        "    df = np.array(X)\n",
        "\n",
        "    dY = np.array(Y)\n",
        "\n",
        "    dataY = dY[T - 1:N]\n",
        "\n",
        "    dataX = np.zeros((N - T + 1, T, D))\n",
        "    for i in range(T, N + 1):\n",
        "        dataX[i - T] = df[i - T:i, :]\n",
        "\n",
        "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
        "\n",
        "def price_legal_check(ask_price, bid_price):\n",
        "    # legal check\n",
        "    ask_price = math.ceil(100*ask_price)/100\n",
        "    bid_price = math.floor(100*bid_price)/100\n",
        "    return ask_price, bid_price\n",
        "\n",
        "def getRealizedVolatility(data, resample='min'):\n",
        "    if resample:\n",
        "        data = data.resample(resample).last()\n",
        "\n",
        "    midprice_lag = data.shift(1)\n",
        "    midprice_log = data.apply(np.log)\n",
        "    midprice_lag_log = midprice_lag.apply(np.log)\n",
        "    r = midprice_log - midprice_lag_log\n",
        "    r2 = r*r\n",
        "    rv = r2.sum()\n",
        "\n",
        "    return rv\n",
        "\n",
        "def getRelativeStrengthIndex(data):\n",
        "    length = len(data)\n",
        "    data = data.resample('s').last()\n",
        "    data = data.pct_change(1)\n",
        "    gain = data[data>0].sum()/length\n",
        "    loss = -data[data<0].sum()/length\n",
        "    if gain or loss:\n",
        "        rsi = gain/(gain+loss)\n",
        "    else:\n",
        "        rsi = .5\n",
        "    return rsi\n",
        "\n",
        "def getOrderStrengthIndex(data):\n",
        "    '''\n",
        "    data: msg\n",
        "    columns:[market_buy_volume  market_buy_n  market_sell_volume  market_sell_n  limit_buy_volume  limit_buy_n  limit_sell_volume  limit_sell_n  withdraw_buy_volume  withdraw_buy_n  withdraw_sell_volume  withdraw_sell_n]\n",
        "    '''\n",
        "    market_volume_intensity = (data.market_buy_volume.sum() - data.market_sell_volume.sum())/(data.market_buy_volume.sum() + data.market_sell_volume.sum() + 1e-7)\n",
        "    market_number_intensity = (data.market_buy_n.sum() - data.market_sell_n.sum())/(data.market_buy_n.sum() + data.market_sell_n.sum() + 1e-7)\n",
        "    limit_volume_intensity = (data.limit_buy_volume.sum() - data.limit_sell_volume.sum())/(data.limit_buy_volume.sum() + data.limit_sell_volume.sum() + 1e-7)\n",
        "    limit_number_intensity = (data.limit_buy_n.sum() - data.limit_sell_n.sum())/(data.limit_buy_n.sum() + data.limit_sell_n.sum() + 1e-7)\n",
        "    withdraw_volume_intensity = (data.withdraw_buy_volume.sum() - data.withdraw_sell_volume.sum())/(data.withdraw_buy_volume.sum() + data.withdraw_sell_volume.sum() + 1e-7)\n",
        "    withdraw_number_intensity = (data.withdraw_buy_n.sum() - data.withdraw_sell_n.sum())/(data.withdraw_buy_n.sum() + data.withdraw_sell_n.sum() + 1e-7)\n",
        "\n",
        "    return market_volume_intensity, market_number_intensity, limit_volume_intensity, limit_number_intensity, withdraw_volume_intensity, withdraw_number_intensity"
      ],
      "metadata": {
        "id": "cW8tTjfx3lWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "import pyrallis\n",
        "from dataclasses import asdict, dataclass\n",
        "\n",
        "from tensorforce.environments import Environment\n",
        "\n",
        "from environment.env_discrete import EnvDiscrete\n",
        "from environment.env_continuous import EnvContinuous\n",
        "from agent.tensorforce_agent import get_dueling_dqn_agent, get_ppo_agent\n",
        "from network.network import *\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    # Experiment\n",
        "    code: str = '000001'\n",
        "    device: str = \"cpu\"\n",
        "    latency: int = 1\n",
        "    time_window: int = 50\n",
        "    log: bool = False\n",
        "    exp_name: str = ''\n",
        "    # Agent\n",
        "    agent_type: str = 'ppo' # ppo/dueling dqn\n",
        "    learning_rate: int = 1e-4\n",
        "    horizon: int = 1\n",
        "    env_type: str = 'continuous' # continuous/discrete\n",
        "    load: bool = False\n",
        "    agent_load_dir: str = ''\n",
        "    save: bool = False,\n",
        "    agent_save_dir: str = ''\n",
        "    # Ablation\n",
        "    wo_pretrain: bool = False\n",
        "    wo_attnlob: bool = False\n",
        "    wo_lob_state: bool = False\n",
        "    wo_market_state: bool = False\n",
        "    wo_dampened_pnl: bool = False\n",
        "    wo_matched_pnl: bool = False\n",
        "    wo_inv_punish: bool = False\n",
        "\n",
        "\n",
        "def init_env(day, config):\n",
        "    if config['env_type'] == 'continuous':\n",
        "        env = EnvContinuous\n",
        "    elif config['env_type'] == 'discrete':\n",
        "        env = EnvDiscrete\n",
        "\n",
        "    environment = env(\n",
        "        code=config['code'],\n",
        "        day=day,\n",
        "        latency=config['latency'],\n",
        "        T=config['time_window'],\n",
        "        # state ablation\n",
        "        wo_lob_state=config['wo_lob_state'],\n",
        "        wo_market_state=config['wo_market_state'],\n",
        "        # reward ablation\n",
        "        wo_dampened_pnl=config['wo_dampened_pnl'],\n",
        "        wo_matched_pnl=config['wo_matched_pnl'],\n",
        "        wo_inv_punish=config['wo_inv_punish'],\n",
        "        # exp setting\n",
        "        experiment_name=config['exp_name'],\n",
        "        log=config['log'],\n",
        "        )\n",
        "    return environment\n",
        "\n",
        "def init_agent(environment, config):\n",
        "    kwargs=dict()\n",
        "    if config['agent_type'] == 'dueling_dqn':\n",
        "        get_agent = get_dueling_dqn_agent\n",
        "        kwargs['learning_rate']=config['learning_rate']\n",
        "        kwargs['horizon']=config['horizon']\n",
        "    elif config['agent_type'] == 'ppo':\n",
        "        get_agent = get_ppo_agent\n",
        "        kwargs['learning_rate']=config['learning_rate']\n",
        "        kwargs['horizon']=config['horizon']\n",
        "\n",
        "    if config['wo_pretrain']:\n",
        "        print(\"Ablation: pretrain\")\n",
        "        lob_model = get_lob_model(64,config['time_window'])\n",
        "        lob_model.compute_output_shape = compute_output_shape\n",
        "    else:\n",
        "        pretrain_model_dir = f'./ckpt/pretrain_model_' + config['code']\n",
        "        model = get_lob_model(64,config['time_window'])\n",
        "        model.compute_output_shape = compute_output_shape\n",
        "        model_pretrain = get_pretrain_model(model,config['time_window'])\n",
        "        checkpoint_filepath = pretrain_model_dir + '/weights'\n",
        "        model_pretrain.load_weights(checkpoint_filepath)\n",
        "        lob_model = model_pretrain.layers[1]\n",
        "\n",
        "    if config['wo_attnlob']:\n",
        "        print(\"Ablation: attnlob\")\n",
        "        lob_model = get_fclob_model(64,config['time_window'])\n",
        "\n",
        "    model = get_model(\n",
        "        lob_model,\n",
        "        config['time_window'],\n",
        "        with_lob_state= not config['wo_lob_state'],\n",
        "        with_market_state= not config['wo_market_state']\n",
        "        )\n",
        "    agent = get_agent(model, environment=environment, max_episode_timesteps=1000, device=config['device'], **kwargs)\n",
        "\n",
        "    if config['load']:\n",
        "        model = keras.models.load_model(keras_model_dir)\n",
        "        model.layers[1].compute_output_shape = compute_output_shape\n",
        "        agent = get_agent(model, environment=environment, max_episode_timesteps=1000, device=config['device'], **kwargs)\n",
        "        agent.restore(config['agent_load_dir'], filename='cppo', format='numpy')\n",
        "\n",
        "    return agent\n",
        "\n",
        "def train_a_day(environment, agent, train_result):\n",
        "    num_episodes = len(environment.orderbook)//num_step_per_episode\n",
        "    data_collector = list()\n",
        "    for idx in tqdm(range(num_episodes)):\n",
        "        episode_states = list()\n",
        "        episode_actions = list()\n",
        "        episode_terminal = list()\n",
        "        episode_reward = list()\n",
        "\n",
        "        states = environment.reset_seq(timesteps_per_episode=num_step_per_episode, episode_idx=idx)\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            episode_states.append(states)\n",
        "            actions = agent.act(states=states, independent=True)\n",
        "            episode_actions.append(actions)\n",
        "            states, terminal, reward = environment.execute(actions=actions)\n",
        "            episode_terminal.append(terminal)\n",
        "            episode_reward.append(reward)\n",
        "\n",
        "        data_collector.append([episode_states, episode_actions, episode_terminal, episode_reward])\n",
        "\n",
        "        agent.experience(\n",
        "            states=episode_states,\n",
        "            actions=episode_actions,\n",
        "            terminal=episode_terminal,\n",
        "            reward=episode_reward\n",
        "        )\n",
        "\n",
        "        agent.update()\n",
        "\n",
        "        save_episode_result(environment, train_result)\n",
        "\n",
        "    return episode_states, episode_actions, episode_reward\n",
        "\n",
        "def test_a_day(environment, agent, test_result):\n",
        "    num_episodes = len(environment.orderbook)//num_step_per_episode\n",
        "    for idx in tqdm(range(num_episodes)):\n",
        "\n",
        "        states = environment.reset_seq(timesteps_per_episode=num_step_per_episode, episode_idx=idx)\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            actions = agent.act(\n",
        "                states=states, independent=True\n",
        "            )\n",
        "            states, terminal, reward = environment.execute(actions=actions)\n",
        "\n",
        "        save_episode_result(environment, test_result)\n",
        "\n",
        "def train(agent, train_result, config):\n",
        "    for day in train_days:\n",
        "        environment = init_env(day, config)\n",
        "        train_a_day(environment, agent, train_result)\n",
        "\n",
        "def test(agent, test_result, config):\n",
        "    for day in test_days:\n",
        "        environment = init_env(day, config)\n",
        "        test_a_day(environment, agent, test_result)\n",
        "\n",
        "def save_episode_result(environment, test_result):\n",
        "    res_dict = environment.get_final_result()\n",
        "    date = environment.day\n",
        "    idx = environment.episode_idx\n",
        "\n",
        "    test_result.loc[date+'_'+str(idx)] = [res_dict['pnl'], res_dict['nd_pnl'], res_dict['avg_abs_position'], res_dict['profit_ratio'], res_dict['volume']]\n",
        "\n",
        "def gather_test_results(test_result):\n",
        "    day_list = list(test_result.index)\n",
        "    for i in range(len(day_list)):\n",
        "        day_list[i] = day_list[i][:10]\n",
        "    day_list = set(day_list)\n",
        "    gathered_results = pd.DataFrame(columns=['PnL', 'ND-PnL', 'average_position', 'profit_ratio', 'volume'])\n",
        "    for day in day_list:\n",
        "        result = test_result[test_result.index.str.contains(day)]\n",
        "        pnl = result.PnL.sum()\n",
        "        nd_pnl = result['ND-PnL'].sum()\n",
        "        ap = result.average_position.mean()\n",
        "        volume = (result.PnL/result.profit_ratio).sum()\n",
        "        pr = pnl/volume\n",
        "        gathered_results.loc[day] = [pnl,nd_pnl,ap,pr,volume]\n",
        "    gathered_results=gathered_results.sort_index()\n",
        "    return gathered_results\n",
        "\n",
        "def save_agent(agent, config):\n",
        "    # save agent network\n",
        "    agent.model.policy.network.keras_model.save(keras_model_dir)\n",
        "    # Save agent\n",
        "    agent.save(config['agent_save_dir'], filename=agent, format='numpy')\n",
        "\n",
        "@pyrallis.wrap()\n",
        "def main(config: TrainConfig):\n",
        "    config = asdict(config)\n",
        "\n",
        "    environment = init_env(train_days[0], config)\n",
        "    agent = init_agent(environment, config)\n",
        "\n",
        "    train_result = pd.DataFrame(columns=['PnL', 'ND-PnL', 'average_position', 'profit_ratio', 'volume'])\n",
        "    for _ in range(n_train_loop):\n",
        "        train(agent, train_result, config)\n",
        "        if config['save']:\n",
        "            save_agent(agent, config)\n",
        "\n",
        "    test_result = pd.DataFrame(columns=['PnL', 'ND-PnL', 'average_position', 'profit_ratio', 'volume'])\n",
        "    test(agent, test_result, config)\n",
        "    daily_test_results = gather_test_results(test_result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_days=['20191101', '20191104', '20191105', '20191106', '20191107', '20191108', '20191111', '20191112']\n",
        "    test_days=['20191113', '20191114', '20191115', '20191118', '20191119', '20191120',\n",
        "            '20191121', '20191122', '20191125', '20191126', '20191127', '20191128', '20191129']\n",
        "    num_step_per_episode = 2000\n",
        "    n_train_loop = 5\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "1bnkJG_F3qVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}